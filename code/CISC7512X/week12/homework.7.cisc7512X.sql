CISC 7512X HW# 7 (practical HW5): In the not-so-distant future, 
flying cars are commonplace---everyone on the planet got one.
Yes, there are ~10 billion flying cars all over the globe. 
Each one logs its coordinates every 10 milliseconds, even when parked. 
Assume x,y,z coordinates, with z being altitude, 
and x,y, some cartesian equivalent of GPS.
To avoid accidents, regulation states that no car can be next to
any other car by more than 10 feet while in the air (z > 0)
for longer than 1 second. Cars can go really fast, ~500mph.

###################### YOUR TASK: ################################
Write an algorithm and program to find all violators.
Assume input is a HUGE file (10 billion cars logging
"VIN,timestamp,x,y,z" every 10 milliseconds all-the-time).
###################################################################
Install Apache Hadoop. [hadoop]. Write a Hive query (or a series of queries),
or a MapReduce program to find all violators (cars that are next 
to other cars while in flight). Assume data is in "cars"
able in Hive (or "/app/cars/data" file on HDFS).
What is the running time of your algorithm? If it's O(N^2),
can you make it run in O(N log N) time? (note that with this
much data, N^2 is not practical, even N log N is a bit long). 
Using your 1 node Hadoop cluster, estimate the amount of 
resources this whole task will consume (to apply it on 10 billion cars),
and put a dollar amount value (assuming it costs $0.10/hour to rent 1 node
(machine); how much will your solution cost per day/month/year?);
rationalize your answer. (note that you can't answer 
"I'll rent 1 node, and let it run until it's done.";
You must process data at least as fast as it is being
generated by all those billions of cars).

Good Hadoop installation guide. Hive installation is 
much simpler, just unzip, set HIVE_HOME,
add bin folder to PATH, and then just run "hive".
Here's some tips on trying to get Hive running 
for first time (links may be outdated).

Submit whatever you create to solve this problem 
(source code for map reduce tasks, or hive queries, etc.,). 
Note, your solution must run (on small dataset)
on a 1-node hadoop cluster. 